RAG News Intelligence API

This project implements a scalable Retrieval-Augmented Generation (RAG) backend designed for answering queries over a news corpus. The system supports ingestion of news articles, vector search, contextual answer generation, and structured logging.

The design follows clean modular architecture principles with clear separation between controllers, services, utilities, and data layers.

ğŸš€ Features
RAG Pipeline

Ingests news articles from a JSON feed.

Generates deterministic embeddings for each article.

Stores them in a simple vector store (easily replaceable with Qdrant/Chroma in production).

Retrieves the most relevant documents using cosine similarity.

Generates contextual answers using a pluggable LLM service (mock LLM included; Gemini/OpenAI can be plugged in).

API Endpoints
Method	Endpoint	Description
POST	/ingest	Ingests and indexes news articles
POST	/chat	Answers a query using retrieval + generation
GET	/history/:sessionId	Fetches logged interactions
DELETE	/history/:sessionId	Clears session history
ğŸ—ï¸ Tech Stack

Node.js (Express)

PostgreSQL / SQLite fallback

Redis (or in-memory fallback)

Docker & Docker Compose

File-based Vector Store (mock)
(Can be swapped with Qdrant/ChromaDB with minimal changes)

ğŸ“‚ Project Structure
rag_news_api/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ controllers/
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ app.js
â”‚   â””â”€â”€ server.js
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ news.json
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ package.json
â”‚â”€â”€ postman_collection.json
â”‚â”€â”€ README.md

âš™ï¸ Installation & Running Locally
1ï¸âƒ£ Install dependencies
npm install

2ï¸âƒ£ Start server
npm start


Server runs at:
http://localhost:5000

ğŸ³ Run Using Docker Compose
docker-compose up --build


This starts:

Node.js API

PostgreSQL

Redis

(You can extend the compose file to include Qdrant for production setup.)

ğŸ§ª Testing the API
Ingest documents
POST http://localhost:5000/ingest

Ask a question
POST http://localhost:5000/chat
{
  "sessionId": "demo-1",
  "query": "Show me tech news"
}

Get history
GET http://localhost:5000/history/demo-1

ğŸ“„ Postman Collection

A Postman collection (postman_collection.json) is included in the project root.
Import it into Postman to test all endpoints.

ğŸ“Š Logging

All interactions are logged to:

PostgreSQL if available

SQLite fallback if not

Each entry stores:

sessionId

user query

model response

response time

timestamp

ğŸ”§ Modularity & Extensibility

This repository is designed to be easily extended:

Replace vector embeddings

Swap embedding.service.js with Jina, HuggingFace, or any embedding model.

Replace vector store

Replace /utils/vectorstore.js with:

Qdrant client

ChromaDB client

Pinecone client

Replace LLM

Swap llm.stub.js with:

Google Gemini

OpenAI

Groq

or any provider

ğŸ“Œ Notes

The project includes a mock LLM for local testing without API keys.

The architecture matches the assignment requirements for modularity, clean design, and containerized execution.

ğŸ“¬ Contact

If you need any help running the project, feel free to reach out.
